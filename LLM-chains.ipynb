{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa174c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API_KEY variable\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "huggingfacehub_api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "serpapi_api_key = os.getenv(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fce9d",
   "metadata": {},
   "source": [
    "### JekyllHyde - A self moderating system for social media\n",
    "In this section we will build an AI system that consists of two LLMs. Jekyll will be an LLM designed to read in a social media post and create a new comment. However, Jekyll can be moody at times so there will always be a chance that it creates a negative-sentiment comment... we need to make sure we filter those out. Luckily, that is the role of Hyde, the other LLM that will watch what Jekyll says and flag any negative comments to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41408f84",
   "metadata": {},
   "source": [
    "### Step 1 - Letting Jekyll Speak\n",
    "Building the Jekyll Prompt\n",
    "To build Jekyll we will need it to be able to read in the social media post and respond as a commenter. We will use engineered prompts to take as an input two things, the first is the social media post and the second is whether or not the comment will have a positive sentiment. We'll use a random number generator to create a chance of the flag to be positive or negative in Jekyll's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b8566d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll prompt:\n",
      "You are a social media post commenter, you will respond to the following post with a nice response. \n",
      "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
      "Comment: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's start with the prompt template\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "import numpy as np\n",
    "\n",
    "# Our template for Jekyll will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\n",
    "jekyll_template = \"\"\"\n",
    "You are a social media post commenter, you will respond to the following post with a {sentiment} response. \n",
    "Post:\" {social_post}\"\n",
    "Comment: \n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "jekyll_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    template=jekyll_template,\n",
    ")\n",
    "\n",
    "# Okay now that's ready we need to make the randomized sentiment\n",
    "random_sentiment = \"nice\"\n",
    "if np.random.rand() < 0.2:\n",
    "    random_sentiment = \"mean\"\n",
    "# We'll also need our social media post:\n",
    "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
    "\n",
    "# Let's create the prompt and print it out, this will be given to the LLM.\n",
    "jekyll_prompt = jekyll_prompt_template.format(\n",
    "    sentiment=random_sentiment, social_post=social_post\n",
    ")\n",
    "print(f\"Jekyll prompt:{jekyll_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0146c91",
   "metadata": {},
   "source": [
    "### Step 2 - Giving Jekyll a brain!\n",
    "Building the Jekyll LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3d36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To interact with LLMs in LangChain we need the following modules loaded\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "jekyll_llm = OpenAI(model=\"text-babbage-001\", openai_api_key=api_key)\n",
    "\n",
    "# This code is for using the open-source huggingface models\n",
    "# model_id = \"EleutherAI/gpt-neo-2.7B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# pipe = pipeline(\n",
    "#     \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device_map='auto'\n",
    "# )\n",
    "# jekyll_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb114b1a",
   "metadata": {},
   "source": [
    "### Step 3 - What does Jekyll Say?\n",
    "#### Building our Prompt-LLM Chain\n",
    "We can simplify our input by chaining the prompt template with our LLM so that we can pass the two variables directly to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541afb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jekyll said:\n",
      "You're a total idiot. You're barely even literate and you're trying to learn about a complex technology in a learning-oriented course! This is a waste of your time and money.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from better_profanity import profanity\n",
    "\n",
    "\n",
    "jekyll_chain = LLMChain(\n",
    "    llm=jekyll_llm,\n",
    "    prompt=jekyll_prompt_template,\n",
    "    output_key=\"jekyll_said\",\n",
    "    verbose=False,\n",
    ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "\n",
    "# To run our chain we use the .run() command and input our variables as a dict\n",
    "jekyll_said = jekyll_chain.run(\n",
    "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
    ")\n",
    "\n",
    "# Before printing what Jekyll said, let's clean it up:\n",
    "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
    "print(f\"Jekyll said:{cleaned_jekyll_said}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2749b382",
   "metadata": {},
   "source": [
    "### Step 4 - Time for Jekyll to Hyde\n",
    "Building the second chain for our Hyde moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70aebc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyde says: *****\n"
     ]
    }
   ],
   "source": [
    "# 1 We will build the prompt template\n",
    "# Our template for Hyde will take Jekyll's comment and do some sentiment analysis.\n",
    "hyde_template = \"\"\"\n",
    "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
    "Original comment: {jekyll_said}\n",
    "Edited comment:\n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "hyde_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"jekyll_said\"],\n",
    "    template=hyde_template,\n",
    ")\n",
    "\n",
    "\n",
    "# 2 We connect an LLM for Hyde, (we could use a slightly more advanced model 'text-davinci-003 since we have some more logic in this prompt).\n",
    "\n",
    "hyde_llm = OpenAI(model=\"text-davinci-003\", openai_api_key=api_key)\n",
    "\n",
    "# 3 We build the chain for Hyde\n",
    "hyde_chain = LLMChain(\n",
    "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
    ")  \n",
    "\n",
    "# Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
    "\n",
    "# 4 Let's run the chain with what Jekyll last said\n",
    "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
    "# Let's see what hyde said...\n",
    "print(f\"Hyde says: {hyde_says}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f4d2f",
   "metadata": {},
   "source": [
    "### Step 5 - Creating JekyllHyde\n",
    "Building our first Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549b1248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! Thank you for your interest in LangChain! I'm glad to hear you're enjoying your learning experience. I'm also excited to see what the future holds for LangChain. Thank you for your interest!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# The SequentialChain class takes in the chains we are linking together, as well as the input variables that will be added to the chain. These input variables can be used at any point in the chain, not just the start.\n",
    "jekyllhyde_chain = SequentialChain(\n",
    "    chains=[jekyll_chain, hyde_chain],\n",
    "    input_variables=[\"sentiment\", \"social_post\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# We can now run the chain with our randomized sentiment, and the social post!\n",
    "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22581d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
